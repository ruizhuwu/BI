{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking1\n",
    "常用的文本分类方法都有哪些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：有传统的机器学习方法，如决策树、SVM、KNN，也有深度学习方法，如TextCNN（将Text的词向量拼接成形如一个channel的图，然后使用CNN）、Bi-LSTM（将Text按正序和反序输入的结果进行拼接，相较单向的LSTM能更好地获取上下文信息）、基于双向Transformer构建的BERT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking2\n",
    "RNN为什么会出现梯度消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：由于RNN损失函数L对参数W求导的结果为：$$ \\frac{\\partial L}{\\partial w} = \\sum_{t=1}^T\\frac{\\partial L}{\\partial y_T}\n",
    "\\frac{\\partial y_T}{\\partial o_T}\\frac{\\partial o_T}{\\partial S_T}\\frac{\\partial S_T}{\\partial S_t}\\frac{\\partial S_t}{\\partial w} $$\n",
    "其中$$ \\frac{\\partial S_T}{\\partial S_t} = \\prod_{k=t+1}^T\\frac{\\partial S_k}{\\partial S_{k-1}} $$\n",
    "而由于$$ S_k = tanh(W_xX_k+W_sS_{k-1}+b) $$\n",
    "$$ tanh'\\in (0,1] $$\n",
    "所以$$ \\frac{\\partial S_T}{\\partial S_t} = \\prod_{k=t+1}^Ttanh'W_s $$ 在$W_s$有界且T很大时，很容易就非常接近于0，从而导致了梯度消失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action1\n",
    "cnews 中文文本分类：\n",
    "由清华大学根据新浪新闻RSS订阅频道2005-2011年间的历史数据筛选过滤生成\n",
    "训练集 50000\n",
    "验证集 5000\n",
    "测试集 10000\n",
    "词汇（字） 5000\n",
    "10个分类，包括：'体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据探索\n",
    "filename = 'cnews.train.txt'\n",
    "with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集变成小样本\n",
    "num_cat = {}\n",
    "num_max = 100 # 每一类最多采100个\n",
    "\n",
    "contents, labels = [], []\n",
    "with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        label, content = line.strip().split('\\t')\n",
    "        #print(label)\n",
    "        if content:\n",
    "            if label not in num_cat:\n",
    "                num_cat[label] = 1\n",
    "                contents.append(content)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                if num_cat[label] < num_max:\n",
    "                    num_cat[label] = num_cat[label] + 1\n",
    "                    contents.append(content)\n",
    "                    labels.append(label)\n",
    "\n",
    "# 写文件\n",
    "with open('cnews.train.small.txt', 'w', encoding='utf-8', errors='ignore') as f:\n",
    "    for content, label in zip(contents, labels):\n",
    "        f.write(label + '\\t' + content+'\\n')\n",
    "    f.close()\n",
    "print(len(contents))\n",
    "print(contents[0])\n",
    "print(labels[0])\n",
    "print(num_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TextRNN(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        # 三个待输入的数据\n",
    "        self.embedding = nn.Embedding(5000, 64)  # 进行词嵌入，5000个词，每个64维\n",
    "#         self.rnn = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, bidirectional=True) # bidirectional指双向的LSTM,输出为128*2维\n",
    "        self.rnn = nn.GRU(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        self.f1 = nn.Sequential(nn.Linear(256,128),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.ReLU())\n",
    "        self.f2 = nn.Sequential(nn.Linear(128,10),\n",
    "                                nn.Softmax(dim=1))\n",
    "        self.dropout = nn.Dropout(p=0.8)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out,_ = self.rnn(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.f1(out[:,-1,:]) #相当于return_sequences=False\n",
    "        out = self.f2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time\n",
    "\n",
    "def train(epochs=100):\n",
    "    model = TextRNN().cuda()\n",
    "    # 定义损失函数\n",
    "    Loss = nn.MultiLabelSoftMarginLoss()\n",
    "    # 定义优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        sum_loss = 0\n",
    "        # 分批训练\n",
    "        for step, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x = x_batch.cuda()\n",
    "            y = y_batch.cuda()\n",
    "            # 前向传播\n",
    "            out = model(x)\n",
    "            loss = Loss(out, y)\n",
    "            sum_loss += loss.item()\n",
    "#             print('loss=', loss)\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).cpu().numpy())\n",
    "#             print('accuracy:', accuracy)\n",
    "            \n",
    "        # 对模型进行验证\n",
    "        if epoch % 5 == 0:\n",
    "            sum_acc = 0\n",
    "            for step, (x_batch, y_batch) in enumerate(val_loader):\n",
    "                x = x_batch.cuda()\n",
    "                y = y_batch.cuda()\n",
    "                # 前向传播\n",
    "                out = model(x)\n",
    "                sum_acc += np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).cpu().numpy())\n",
    "            if sum_acc > best_val_acc:\n",
    "                torch.save(model, 'model.pkl')\n",
    "                best_val_acc = sum_acc\n",
    "                print('model.pkl saved')\n",
    "        \n",
    "        print('epoch{} sum of loss{: .4f} time{: .4f}'.format(epoch, sum_loss, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval() # 固定参数\n",
    "    correct, total = 0, 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x = x_batch.cuda()\n",
    "        y = y_batch.cuda()\n",
    "        # 前向传播\n",
    "        out = model(x)\n",
    "        # 预测结果\n",
    "        _, pred = torch.max(out.data, dim=1)# 取评分最高的类(dim=0是每列的最大值，dim=1是每行的最大值，返回两个值，一个是最大值tensor组一个是最大值所在的位置)\n",
    "        _, label = torch.max(y.data, dim=1)\n",
    "        # 对比结果与真实值\n",
    "        total += y.size()[0]\n",
    "        correct += (pred==label).sum().item()\n",
    "    print('测试准确率为：{: .4f}%'.format(100*correct/total))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据目标文件名\n",
    "train_file = 'cnews.train.txt'\n",
    "test_file = 'cnews.test.txt'\n",
    "val_file = 'cnews.val.txt'\n",
    "vocab_file = 'cnews.vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
      "x_train= [[1609  659   56 ...    9  311    3]\n",
      " [   2  101   16 ... 1168    3   24]\n",
      " [ 465  855  521 ...  116  136   85]\n",
      " ...\n",
      " [  49   18   79 ...  836 1928 1072]\n",
      " [ 166  110  714 ...  836 1928 1072]\n",
      " [   1   80  551 ...   78  192    3]]\n",
      "['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
      "(50000, 600)\n",
      "x_train: [[1609  659   56 ...    9  311    3]\n",
      " [   2  101   16 ... 1168    3   24]\n",
      " [ 465  855  521 ...  116  136   85]\n",
      " ...\n",
      " [  49   18   79 ...  836 1928 1072]\n",
      " [ 166  110  714 ...  836 1928 1072]\n",
      " [   1   80  551 ...   78  192    3]]\n"
     ]
    }
   ],
   "source": [
    "from cnews_loader import read_vocab, read_category, process_file\n",
    "# 获取文本的类别及其对应id的字典\n",
    "categories, cat_to_id = read_category()\n",
    "print(categories)\n",
    "# 获取训练文本中所出现过的字及其对应的id\n",
    "words, word_to_id = read_vocab('cnews.vocab.txt')\n",
    "# print(words)\n",
    "# 获取训练书记每一个字的id和对应标签的one_hot编码\n",
    "x_train, y_train = process_file(train_file, word_to_id, cat_to_id, 600)\n",
    "print(x_train.shape)\n",
    "print('x_train:', x_train)\n",
    "x_val, y_val = process_file(val_file, word_to_id, cat_to_id, 600)\n",
    "x_test, y_test = process_file(test_file, word_to_id, cat_to_id, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pkl saved\n",
      "epoch0 sum of loss 562.8416 time 180.9854\n",
      "epoch1 sum of loss 559.7274 time 175.8596\n",
      "epoch2 sum of loss 559.2398 time 175.7991\n",
      "epoch3 sum of loss 558.9432 time 175.6949\n",
      "epoch4 sum of loss 558.6346 time 175.8098\n",
      "model.pkl saved\n",
      "epoch5 sum of loss 558.6075 time 181.1315\n",
      "epoch6 sum of loss 558.4473 time 174.9646\n",
      "epoch7 sum of loss 558.4816 time 174.9964\n",
      "epoch8 sum of loss 558.3392 time 175.0742\n",
      "epoch9 sum of loss 558.2554 time 175.0573\n",
      "model.pkl saved\n",
      "epoch10 sum of loss 558.2256 time 180.4739\n",
      "epoch11 sum of loss 558.1895 time 130.4348\n",
      "epoch12 sum of loss 558.0478 time 36.5736\n",
      "epoch13 sum of loss 557.9568 time 36.5107\n",
      "epoch14 sum of loss 557.8954 time 36.4607\n",
      "model.pkl saved\n",
      "epoch15 sum of loss 557.8702 time 37.5322\n",
      "epoch16 sum of loss 557.8825 time 36.4056\n",
      "epoch17 sum of loss 557.6916 time 36.4302\n",
      "epoch18 sum of loss 557.7475 time 36.4511\n",
      "epoch19 sum of loss 557.6788 time 36.3253\n",
      "model.pkl saved\n",
      "epoch20 sum of loss 557.6378 time 37.4997\n",
      "epoch21 sum of loss 557.5925 time 36.3934\n",
      "epoch22 sum of loss 557.5813 time 36.3740\n",
      "epoch23 sum of loss 557.5475 time 36.3867\n",
      "epoch24 sum of loss 557.4974 time 36.4163\n",
      "model.pkl saved\n",
      "epoch25 sum of loss 557.5299 time 37.5346\n",
      "epoch26 sum of loss 557.5251 time 36.3938\n",
      "epoch27 sum of loss 557.4202 time 36.4490\n",
      "epoch28 sum of loss 557.4352 time 36.5246\n",
      "epoch29 sum of loss 557.4515 time 36.3916\n",
      "epoch30 sum of loss 557.4579 time 37.6823\n",
      "epoch31 sum of loss 557.4212 time 36.3705\n",
      "epoch32 sum of loss 557.4067 time 36.4232\n",
      "epoch33 sum of loss 557.4074 time 36.5130\n",
      "epoch34 sum of loss 557.3612 time 36.4476\n",
      "epoch35 sum of loss 557.3510 time 37.5618\n",
      "epoch36 sum of loss 557.3531 time 36.5778\n",
      "epoch37 sum of loss 557.3789 time 36.3829\n",
      "epoch38 sum of loss 557.3229 time 36.3523\n",
      "epoch39 sum of loss 557.3135 time 36.4550\n",
      "epoch40 sum of loss 557.2806 time 37.5282\n",
      "epoch41 sum of loss 557.2931 time 36.4452\n",
      "epoch42 sum of loss 557.2671 time 37.1050\n",
      "epoch43 sum of loss 557.3086 time 36.7920\n",
      "epoch44 sum of loss 557.2950 time 36.6490\n",
      "model.pkl saved\n",
      "epoch45 sum of loss 557.2624 time 37.6922\n",
      "epoch46 sum of loss 557.2909 time 36.7409\n",
      "epoch47 sum of loss 557.2983 time 36.6231\n",
      "epoch48 sum of loss 557.3072 time 36.5742\n",
      "epoch49 sum of loss 557.2905 time 36.6140\n",
      "epoch50 sum of loss 557.3086 time 37.6920\n",
      "epoch51 sum of loss 557.2906 time 36.3506\n",
      "epoch52 sum of loss 557.3540 time 36.6928\n",
      "epoch53 sum of loss 557.2812 time 36.3419\n",
      "epoch54 sum of loss 557.3507 time 36.3226\n",
      "epoch55 sum of loss 557.2965 time 37.4515\n",
      "epoch56 sum of loss 557.2794 time 36.4816\n",
      "epoch57 sum of loss 557.2792 time 36.4325\n",
      "epoch58 sum of loss 557.2564 time 36.4168\n",
      "epoch59 sum of loss 557.2862 time 36.5150\n",
      "epoch60 sum of loss 557.2708 time 37.5490\n",
      "epoch61 sum of loss 557.3105 time 36.6749\n",
      "epoch62 sum of loss 557.2342 time 36.6907\n",
      "epoch63 sum of loss 557.2851 time 36.5920\n",
      "epoch64 sum of loss 557.2407 time 36.6128\n",
      "epoch65 sum of loss 557.2393 time 37.6685\n",
      "epoch66 sum of loss 557.2590 time 36.6720\n",
      "epoch67 sum of loss 557.3064 time 36.6287\n",
      "epoch68 sum of loss 557.2595 time 36.6112\n",
      "epoch69 sum of loss 557.2828 time 36.6650\n",
      "model.pkl saved\n",
      "epoch70 sum of loss 557.2506 time 37.7963\n",
      "epoch71 sum of loss 557.2858 time 36.7169\n",
      "epoch72 sum of loss 557.2821 time 36.6890\n",
      "epoch73 sum of loss 557.2186 time 36.5847\n",
      "epoch74 sum of loss 557.2922 time 35.9852\n",
      "epoch75 sum of loss 557.2423 time 36.9558\n",
      "epoch76 sum of loss 557.2658 time 35.9997\n",
      "epoch77 sum of loss 557.2056 time 36.0841\n",
      "epoch78 sum of loss 557.2033 time 36.1416\n",
      "epoch79 sum of loss 557.1976 time 36.0940\n",
      "epoch80 sum of loss 557.2389 time 37.2359\n",
      "epoch81 sum of loss 557.2061 time 35.9761\n",
      "epoch82 sum of loss 557.2163 time 36.1000\n",
      "epoch83 sum of loss 557.2165 time 36.3451\n",
      "epoch84 sum of loss 557.2730 time 36.2415\n",
      "epoch85 sum of loss 557.2126 time 37.1702\n",
      "epoch86 sum of loss 557.2580 time 36.1279\n",
      "epoch87 sum of loss 557.2092 time 36.0096\n",
      "epoch88 sum of loss 557.2376 time 36.0412\n",
      "epoch89 sum of loss 557.2047 time 36.0826\n",
      "epoch90 sum of loss 557.2360 time 37.1568\n",
      "epoch91 sum of loss 557.2224 time 36.1210\n",
      "epoch92 sum of loss 557.2254 time 36.0858\n",
      "epoch93 sum of loss 557.2139 time 36.0227\n",
      "epoch94 sum of loss 557.2153 time 36.0568\n",
      "epoch95 sum of loss 557.2222 time 37.2194\n",
      "epoch96 sum of loss 557.1899 time 36.1549\n",
      "epoch97 sum of loss 557.1966 time 36.1702\n",
      "epoch98 sum of loss 557.2016 time 36.0790\n",
      "epoch99 sum of loss 557.1543 time 36.1320\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "# 设置GPU\n",
    "cuda = torch.device('cuda')\n",
    "x_train, y_train = torch.LongTensor(x_train), torch.Tensor(y_train)\n",
    "x_val, y_val = torch.LongTensor(x_val), torch.Tensor(y_val)\n",
    "x_test, y_test = torch.LongTensor(x_test), torch.Tensor(y_test)\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = Data.TensorDataset(x_val, y_val)\n",
    "val_loader = Data.DataLoader(dataset=val_dataset, batch_size=64)\n",
    "test_dataset = Data.TensorDataset(x_test, y_test)\n",
    "test_loader = Data.DataLoader(dataset=test_dataset, batch_size=64)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试准确率为： 40.6500%\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load('model.pkl')\n",
    "test(best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
