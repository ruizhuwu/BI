{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking1\n",
    "常用的文本分类方法都有哪些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：有传统的机器学习方法，如决策树、SVM、KNN，也有深度学习方法如TextCNN（将Text的词向量拼接成形如一个channel的图，然后使用CNN）、Bi-LSTM（将Text按正序和反序输入的结果进行拼接，相较单向的LSTM能更好地获取上下文信息）、基于双向Transformer构建的BERT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking2\n",
    "RNN为什么会出现梯度消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action1\n",
    "cnews 中文文本分类：\n",
    "由清华大学根据新浪新闻RSS订阅频道2005-2011年间的历史数据筛选过滤生成\n",
    "训练集 50000\n",
    "验证集 5000\n",
    "测试集 10000\n",
    "词汇（字） 5000\n",
    "10个分类，包括：'体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据探索\n",
    "filename = 'cnews.train.txt'\n",
    "with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集变成小样本\n",
    "num_cat = {}\n",
    "num_max = 100 # 每一类最多采100个\n",
    "\n",
    "contents, labels = [], []\n",
    "with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        label, content = line.strip().split('\\t')\n",
    "        #print(label)\n",
    "        if content:\n",
    "            if label not in num_cat:\n",
    "                num_cat[label] = 1\n",
    "                contents.append(content)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                if num_cat[label] < num_max:\n",
    "                    num_cat[label] = num_cat[label] + 1\n",
    "                    contents.append(content)\n",
    "                    labels.append(label)\n",
    "\n",
    "# 写文件\n",
    "with open('cnews.train.small.txt', 'w', encoding='utf-8', errors='ignore') as f:\n",
    "    for content, label in zip(contents, labels):\n",
    "        f.write(label + '\\t' + content+'\\n')\n",
    "    f.close()\n",
    "print(len(contents))\n",
    "print(contents[0])\n",
    "print(labels[0])\n",
    "print(num_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TextRNN(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        # 三个待输入的数据\n",
    "        self.embedding = nn.Embedding(5000, 64)  # 进行词嵌入，5000个词，每个64维\n",
    "        self.rnn = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, bidirectional=True) # bidirectional指双向的LSTM,输出为128*2维\n",
    "#         self.rnn = nn.GRU(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        self.f1 = nn.Sequential(nn.Linear(256,128),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.ReLU())\n",
    "        self.f2 = nn.Sequential(nn.Linear(128,10),\n",
    "                                nn.Softmax(dim=1))\n",
    "        self.dropout = nn.Dropout(p=0.8)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out,_ = self.rnn(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.f1(out[:,-1,:]) #相当于return_sequences=False\n",
    "        out = self.f2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def train(epochs=100):\n",
    "    model = TextRNN().cuda()\n",
    "    # 定义损失函数\n",
    "    Loss = nn.MultiLabelSoftMarginLoss()\n",
    "    # 定义优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch=', epoch)\n",
    "        # 分批训练\n",
    "        for step, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x = x_batch.cuda()\n",
    "            y = y_batch.cuda()\n",
    "            # 前向传播\n",
    "            out = model(x)\n",
    "            loss = Loss(out, y)\n",
    "            print('loss=', loss)\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).cpu().numpy())\n",
    "#             print('accuracy:', accuracy)\n",
    "            \n",
    "        # 对模型进行验证\n",
    "        if epoch % 5 == 0:\n",
    "            sum_acc = 0\n",
    "            for step, (x_batch, y_batch) in enumerate(val_loader):\n",
    "                x = x_batch.cuda()\n",
    "                y = y_batch.cuda()\n",
    "                # 前向传播\n",
    "                out = model(x)\n",
    "                sum_acc += np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).cpu().numpy())\n",
    "            if sum_acc > best_val_acc:\n",
    "                torch.save(model, 'model.pkl')\n",
    "                best_val_acc = sum_acc\n",
    "                print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval() # 固定参数\n",
    "    correct, total = 0, 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x = x_batch.cuda()\n",
    "        y = y_batch.cuda()\n",
    "        # 前向传播\n",
    "        out = model(x)\n",
    "        # 预测结果\n",
    "        _, pred = torch.max(out.data, dim=1)# 取评分最高的类(dim=0是每列的最大值，dim=1是每行的最大值，返回两个值，一个是最大值tensor组一个是最大值所在的位置)\n",
    "        _, label = torch.max(y.data, dim=1)\n",
    "        # 对比结果与真实值\n",
    "        total += y.size()[0]\n",
    "        correct += (pred==label).sum().item()\n",
    "    print('测试准确率为：{: .4f}%'.format(100*correct/total))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据目标文件名\n",
    "train_file = 'cnews.train.txt'\n",
    "test_file = 'cnews.test.txt'\n",
    "val_file = 'cnews.val.txt'\n",
    "vocab_file = 'cnews.vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
      "(50000, 600)\n",
      "x_train: [[1609  659   56 ...    9  311    3]\n",
      " [   2  101   16 ... 1168    3   24]\n",
      " [ 465  855  521 ...  116  136   85]\n",
      " ...\n",
      " [  49   18   79 ...  836 1928 1072]\n",
      " [ 166  110  714 ...  836 1928 1072]\n",
      " [   1   80  551 ...   78  192    3]]\n"
     ]
    }
   ],
   "source": [
    "from cnews_loader import read_vocab, read_category, process_file\n",
    "# 获取文本的类别及其对应id的字典\n",
    "categories, cat_to_id = read_category()\n",
    "print(categories)\n",
    "# 获取训练文本中所出现过的字及其对应的id\n",
    "words, word_to_id = read_vocab('cnews.vocab.txt')\n",
    "# print(words)\n",
    "# 获取训练书记每一个字的id和对应标签的one_hot编码\n",
    "x_train, y_train = process_file(train_file, word_to_id, cat_to_id, 600)\n",
    "print(x_train.shape)\n",
    "print('x_train:', x_train)\n",
    "x_val, y_val = process_file(val_file, word_to_id, cat_to_id, 600)\n",
    "x_test, y_test = process_file(test_file, word_to_id, cat_to_id, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "# 设置GPU\n",
    "cuda = torch.device('cuda')\n",
    "x_train, y_train = torch.LongTensor(x_train), torch.Tensor(y_train)\n",
    "x_val, y_val = torch.LongTensor(x_val), torch.Tensor(y_val)\n",
    "x_test, y_test = torch.LongTensor(x_test), torch.Tensor(y_test)\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = Data.TensorDataset(x_val, y_val)\n",
    "val_loader = Data.DataLoader(dataset=val_dataset, batch_size=64)\n",
    "test_dataset = Data.TensorDataset(x_test, y_test)\n",
    "test_loader = Data.DataLoader(dataset=test_dataset, batch_size=64)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试准确率为： 41.8700%\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load('model.pkl')\n",
    "test(best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
